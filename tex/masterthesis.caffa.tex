\chapter{CAFFA Framework}
\label{sec:caffa}

This chapter presents the CAFFA framework that has been extended for the present thesis. \emph{CAFFA} is an acronym, which stands for \emph{Computer Aided Fluid Flow Analysis}. The solver framework is based on Peri\'c's CAFFA code \cite{ferziger02} which has been modified to handle three-dimensional problem domains and block-structured grids with non-matching blocks. Furthermore, the framework has been parallelized making extensive use of the PETSc library. One characteristic of the solver framework is that arbitrary block boundaries are handled in a fully implicit way. Details on how those transitional conditions are handled can be found in section \ref{sec:blockboundaries}. The framework consists of two different solver algorithms, a segregated solver and a fully coupled solver, whose theoretical basis has been introduced in chapters \ref{sec:seg} and \ref{sec:cpld} respectively. In addition, the framework features an own implementation of a grid generator for block-structured grids as well as a mapper program to convert grids generated by a commercial meshing tool into a format understandable by the CAFFA code. To prepare the grid data for later use in the solvers, a preprocessor program has been integrated into the framework. Furthermore, the solver programs exhibit different export functionalities to visualize the numerical grid and the results within ParaView \cite{paraview} and the export of binary vectors and matrices to MATLAB \textregistered \cite{matlab}.

The following sections will now briefly introduce the PETSc framework and present the components of the CAFFA framework.

\section{PETSc Framework}
\label{sec:petsc}

\emph{PETSc} is an acronym for \emph{Portable Extensible Toolkit for Scientific Computation} and is a software suite which comprises data structures and an extensive set of linear solver routines \cite{petsc-web-page,petsc-efficient}. A great part of the data structures is designed to be used for parallelized applications on high-performance computers. Also, the solver routines have been parallelized using the MPI standard for message passing. PETSc provides tools which can be used to build large-scale application codes for scientific calculations. The great advantage of PETSc is that it comprises multiple additional layers of abstraction. This abstraction makes the use of PETSc in applications straightforward and allows the users to focus on the essential implementations of their own code instead of having to deal with parallel data structures and algorithms, since this represents an additional source of programming errors. PETSc provides different interfaces through which the user can parallelize his code, what furthermore increases readability and maintainability of the developed code. Last but not least, the implementations used for data structures and solvers in PETSc are not only verified and updated on a regular basis, they have furthermore proven to be highly efficient in the creation of large-scale computer applications on high-performance clusters \cite{bonfiglioli12,gropp00,karimian05}.

The data structures of PETSc reflect the commonly used elements in scientific codes. Vectors, matrices, and index sets are among these basic data structures. Beyond that, PETSc offers a variety of options for its data structures. Vectors can either be used sequentially or in parallel, in which case PETSc distributes the vector components to the involved processes inside a specified MPI communicator. Furthermore, parallel matrix formats are available that allow the processes to assemble different parts of one global matrix simultaneously. PETSc contains other data structures which can be used to fetch and distribute either vector or matrix elements from remote processes.

On the other side, PETSc contains a large collection of parallel solvers for linear systems. The major part comprises Krylov subspace methods. An overview over classical Krylov subspace methods can be found in \cite{saad03} Among the variety of solver options that are available for further optimization of the performance, the Krylov subspace methods can be combined with elements of another thorough collection of preconditioners and other third party packages like Hypre \cite{hypre} or ML \cite{ml}. For further details on the available data structures and subroutines, the reader is referred to \cite{petsc-user-ref,petsc-web-page}.

In addition to the mentioned tools, PETSc also comes with an internal profiler tool which collects a variety of information on the used PETSc objects and bundles them into a human readable log file. The log file and other dynamic output, which can be triggered by command line arguments, facilitate the identification of performance bottlenecks and accelerate the optimization process of developed applications.

\section{Grid Generation and Preprocessing}
\label{sec:gridpreproc}

A grid generator has been developed to generate hexahedral block-structured grids. To provide flexibility for testing locally refined and skewed grids, the grid generator deploys a random number generator which moves grid points within the domain and varies the number of grid cells of each block optionally. Key feature of the developed framework is the handling of block-structured locally refined grids with non-matching block interfaces. The neighboring relations are represented by a special type of boundary condition.

After the grid has been generated, a preprocessor program comes to application which prepares the numerical grid for the use in the solver program. An essential part of the preprocessing step is the application of various matching algorithms depending on the type of block boundary condition. The preprocessor program differentiates between three block boundary types of which each is a subtype of the other. Figure \ref{fig:matching} illustrates these different types of block boundaries and shows the resulting relation between block boundary cells. The most simple block boundary condition is given by a \emph{one-to-one} relation between two neighboring boundary cells. The generalization of this condition involves several boundary cells that share only one neighboring boundary cell in the neighboring block. This relation is also known as a \emph{many-to-one} relation. The third boundary condition on block boundaries implements the most general \emph{many-to-many} relation, which needs special consideration in the preprocessing step, because the number of neighbors one cell can have is not known a priori. What makes the third type of boundary conditions more difficult than the first two, is the geometry of the resulting boundary faces. The current implementation uses a polygon clipper to calculate the polygons resulting from the intersection of two boundary faces of neighboring blocks. Based on the polygon of the intersection, all further geometric data like interpolation factors and boundary face area can be calculated for the later use within the solver application.

\begin{figure}[h!]
  \centering
  \input{./img/matching.oto.tikz.tex}
  \caption{Different types of block boundaries for a two-dimensional block-structured grid}
  \label{fig:matching}
\end{figure}

\section{Implementation of CAFFA}
\label{sec:caffaimpl}

This section provides an overview of the implementation aspects of the developed CAFFA framework. Since applications for modern computers have to be able to use the provided resources efficiently, the concept for the parallelization of the application is presented. A separate section presents, how convergence is monitored and controlled. Later on, this section shows how the PETSc data structures that store the variables are used to realize the data composition as a central part of the parallelization process.

\subsection{The Message-Passing Model}

The PETSc framework makes use of the message-passing computational model \cite{gropp99}. This model assumes that a set of processes with local memory is able to communicate with other processes by passing, i.e. sending and receiving, messages. \emph{MPI} is an acronym for \emph{Message Passing Interface} and is a software library that collects features of message-passing systems. More importantly, MPI provides a widely used standard that assures the portability of applications that make use of this library, as does PETSc. 

The message-passing model is not only reflected in the application as a programming paradigm, but also in the design of parallel computers and their communication network. From this perspective, the message-passing model is an analog to the distributed memory design which encapsulates the idea of local memory and the exchange of data through a network. High-performance computers are seldom designed exclusively as a distributed memory system. They rather additionally incorporate features of another model, namely the shared memory model. This memory model is characterized by a common address space which each of the processes can access uniformly, i.e. with a location independent latency, or non-uniformly. Latter systems are also called \emph{NUMA} (\emph{Non-Uniform Memory Access}) systems. A widely used system design, as used in the system the performance tests of chapter \ref{sec:compare} are carried out on, comprises features of both models by using NUMA computing nodes that share local memory and are able to exchange data with other NUMA nodes via an interconnect and the passing of messages.

One important advantage of using software that uses the message passing model and hence applications that were parallelized within this programming paradigm is their compatibility with computers that use combinations of distributed and shared memory.

\subsection{Convergence Control} 
\label{sec:convergence}
One part of the PETSc philosophy is to provide maximum flexibility in choosing solvers and parameters from within the command line. However, the developed framework implements a nonlinear iteration process without using the \emph{SNES} (\emph{Scalable Nonlinear Equations Solvers}) objects provided by PETSc. This fact creates the need for a hard-coded implementation of convergence control. Convergence control for the used Picard iteration method comprises two parts. One part controls the overall convergence and determines when the calculations have finished. The other part controls the convergence on an outer iteration basis. The convergence criteria are met if the actual residual falls below the upper bound for the residual \(r_{final}\). The bound for final convergence is determined to
\begin{displaymath}
  r_{final} = r_{initial} * 10^{-8},
\end{displaymath}
which states that a relative decrease in the initial residual of \(10^{-8}\) indicates convergence. This criterion will be fixed for all further analyses since it controls the overall accuracy of the solver results and hence maintains comparability of solver performance, unless otherwise stated. 

The other convergence criterion can be handled with more flexibility since it controls the relative decrease of the residual in each outer iteration. This parameter should not affect the overall accuracy but instead convergence speed, within the individual bounds on the solver algorithm. Low relative solver tolerances will not always benefit convergence speed. While high relative solver tolerances will decrease the number of needed inner iterations, the number of outer iterations might increase nullifying the benefit of faster convergence of the linear solver. The convergence criterion for each outer iteration is implemented as
\begin{displaymath}
  r_{final}^{(k)} = r_{initial}^{(k)} * \operatorname{rtol},
\end{displaymath}
where \(\operatorname{rtol}\) indicates the relative decrease of the residual in each outer iteration. 

\subsection{Indexing of Variables and Treatment of Boundary Values}

All variables needed by the CAFFA solver are represented by one-dimensional arrays. To establish a mapping between a location \((i,j,k)\) of the numerical grid and the corresponding location \((ijk)\) inside a variable's array the following formula is used
\begin{displaymath}
  (ijk) = N_i \, N_j \left(k - 1\right) + N_j \left(i - 1\right) + j,
\end{displaymath}
where \(N_i\) and \(N_j\) are the numbers of grid cells in the respective coordinate direction plus two additional boundary cells. The inclusion of the boundary cells circumvents the need to provide an additional data structure for boundary values. Furthermore, the same mapping rule can be used for the grid vertex coordinates. This indexing of variables will be used to assemble the matrices surging from the discretization process and lead to rows for the boundary values that are decoupled from the rest of the linear system. PETSc provides two different approaches to handle boundary values. In the first approach another abstraction layer is introduced via an object which redistributes the data and removes the rows containing the boundary values. This approach has been tested with the conclusion that the redistribution causes not only significant overhead but also hides important data for debugging solver convergence. In the present work, a second approach is used which retains boundary values and adapts the right-hand side accordingly. This approach has proven to be more efficient with the drawback of memory overhead and the need to reset the boundary values after a linear system has been solved with a high relative tolerance.

The presented indexing model is not capable of handling block-structured grids or grids that have been distributed across various processes within the MPI programming model. This property is implemented by additional mappings that return a constant offset \((ijk)_b\) for each block and \((ijk)_p\) for each process
\begin{displaymath}
  (ijk) = (ijk)_p + (ijk)_b +  N_i \, N_j \left(k - 1\right) + N_j \left(i - 1\right) + j.
\end{displaymath}
The introduction of the process offset is needed to provide a mapping between locally and globally indexed values. Within one process only the local indexing 
\begin{displaymath}
  (ijk)_{loc} = (ijk)_b +  N_i \, N_j \left(k - 1\right) + N_j \left(i - 1\right) + j
\end{displaymath}
will be used, since these indexes can be mapped directly to memory addresses. For inter-process communication, however, global indices have to be provided to the respective PETSc subroutines by adding the process offset
\begin{equation}
  \label{eq:globalmap}
  (ijk)_{glo} = (ijk)_{loc} + (ijk)_p.
\end{equation}
It should be noted that the presented mappings do not include the index mappings from FORTRAN applications, which use \(1\)-based indexing, to the PETSc subroutines, which use \(0\)-based indices.

As presented in chapter \ref{sec:cpld}, the use of a coupling algorithm leads to a global linear system that contains the linear algebraic equations for different variables. The implementation used for the present thesis interlaces these variables, a fact that increases the locality of the data and hence improves memory efficiency. The presented mapping is easily extended to handle arrays of values that contain \(n_{eq}\) variables and use an interlaced storing approach by multiplying (\ref{eq:globalmap}) with the number of interlaced values
\begin{displaymath}
  (ijk)_{interglo} = n_{eq} * \left( (ijk)_{glo}  - 1 \right) + c_{eq},
\end{displaymath}
where \(c_{eq} = 1,\dots,n_{eq} \).

\subsection{Domain Decomposition and the Exchange of Ghost Values}

The developed CAFFA framework is able to solve a given problem across different processes. For this, before each solve, the relevant data has to be distributed among all involved processes. Within the solver parallelization, three types of this data distribution are considered with respect to their frequency of communication and redistribution among all or only a part of the processes involved in the solution procedure. 

The first type of data refers to the distribution of stationary data that will not change throughout the solution process. This type of data refers mostly to problem geometry-related data, as for example the coordinates of the grid points. The distribution of the data is already part of the domain decomposition process and can be done before the solver application starts, as it is implemented in the developed solver framework. In the case, in which local dynamic refinement strategies are used, the geometric data at block boundaries will have to be redistributed. If additionally dynamic load balancing is used, even geometric data from inside the blocks of the problem domain might have to be redistributed. This use case, however, imposes other design guidelines for the involved processes which lay outside of the scope of the present thesis.

The distribution of stationary data takes place throughout the preprocessing program within the developed framework before the CAFFA solver is launched. Each processor is assigned a binary file with the respective data. Since the solver is not able to handle adaptive grid refinement or dynamic load balancing, this approach is straightforward.

The second type of data has to be distributed regularly since this data changes at least every outer iteration. The necessity to interchange values, also known as \emph{ghosting}, between processors surges when calculations of variable gradients or coefficients are made for block boundary control volumes. PETSc provides special interfaces for ghosted vectors that allow to perform different types of vector updates through top-level subroutines. The central part are the involved vector scatter and gather routines which allow to calculate contributions to source terms of control volumes located at block boundaries or to update ghost values for the control volumes that rely on variable values from neighboring blocks to calculate fluxes or gradients. The present work developed an approach for ghosting values which is not symmetric in the sense that each block saves ghost values of the neighboring blocks. On the contrary, for each block boundary only one of the neighbored blocks is responsible for the calculation of the fluxes and gradient contributions. This convention maintains the same amount of data communicated through the interconnect while splitting the computational effort almost in half.

Another advantage of the PETSc environment for ghosting values that reside in the memory space of other processes is the unified approach of data coherence, since on the top level, only one vector object to store all data related to a variable, e.g. the velocity \(u_1\), has to be created. Through precise information provided in the creation step of these vector objects, PETSc distributes the vector object to all involved processes and uses the provided information to manage the ghosting through the PETSc internal vector scatter and gather routines. 

The present solver framework treats the calculation of matrix coefficients for block boundary control volumes in a special way. To reduce communication overhead and redundant data, only one of the two involved processors calculates these coefficients and then sends them to the respective neighbor when the matrix is being assembled. The data needed to calculate the matrix coefficients of a neighboring process embraces not only the values of the dominant variables from the last outer iteration but also the values of the cell center gradients. For the ghosting of variable values, PETSc offers special vector types which comprise a user-friendly interface to use the gather and scatter routines provided by PETSc. During the creation of these vectors, a set of global indices referring to the values that are to be ghosted, has to be provided. After this, the interchange of values is realized through a single subroutine call to \textrm{VecGhostUpdate}, since PETSc handles the communication process which is necessary to accomplish the passing of values. Hence, the local representation of a ghosted vector, not only comprises the data local to one processor, but also provides space for ghosted values that are updated repeatedly. The parallel vector layout and the process of ghosting are schematically shown in Figure \ref{fig:ghosting} for a two-dimensional domain consisting of two grid blocks distributed among two processes.

\begin{figure}[h!]
  \centering
  \input{./img/ghosting.tikz.tex}
  \caption{Storage and update of ghost values in the vectors related to the variables on multi-block domains. The blocks have been assigned to two different processes, \emph{Proc} $1$ and \emph{Proc} $2$. The control volumes of the two-dimensional problem domain are indexed with respect to the process local indexing.}
  \label{fig:ghosting}
\end{figure}

The last type of data distribution refers to global reduce operations, in which a single process gathers data from all other processes and, after some modification, redistributes the data. A common scenario for global reduce operations is the calculation of the mean pressure throughout the domain which has been introduced in section \ref{sec:singularitytreatment}.


